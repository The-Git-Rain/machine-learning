# 第四章决策树
熵 事物的不确定  
信息 消除事物的不确定性 调整概率 排除干扰  确定情况   
噪音 不能消除的事物的不确定性  
熵的量化 bit 一次抛硬币 两种可能 因此为1个bit  8这种等概率结果为3bit  
一般分布 plog2(1/p)合起来就是总熵  
信息提供提供的信息就等于熵的差额即提供前的减去提供后的  
信息增益对取值较多的分类有偏好  
增益率熵差除以增加信息后的熵？  
CART算法基尼指数 纯度为任取两样本 该两样本是同一类的概率，基尼系数是1减去纯度  
总的基尼系数就是加权平均 基尼系数越大越不纯 越小 越纯  
基尼系数变小就可分  
剪枝处理 防止过拟合  预剪枝->根据测试集正确率是否变高检测是否还需要分叉   
后剪枝就是自下而上的看  预剪枝就自顶向下的看  
缺失值的处理 比如 17个样例 最后只有14个有值新的信息增益即为14/17×算出来的信息增益  或者相关性最高的填充
